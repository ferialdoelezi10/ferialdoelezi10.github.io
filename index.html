<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Smartville Documentation</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }

        h1 {
            font-size: 3em;
            color: #0056b3;
            text-align: center;
            margin-top: 0;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        h2,
        h3,
        h4 {
            color: #0056b3;
        }

        p {
            line-height: 1.6;
        }

        pre {
            background-color: #1e1e1e;
            color: #d4d4d4;
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            text-align: left;
            white-space: pre-wrap;
        }

        pre code {
            display: block;
            font-family: 'Courier New', Courier, monospace;
            white-space: pre;
        }

        .keyword {
            color: #569cd6;
        }

        .builtin {
            color: #4ec9b0;
        }

        .string {
            color: #d69d85;
        }

        .comment {
            color: #6a9955;
        }

        .number {
            color: #b5cea8;
        }

        a {
            color: #0056b3;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
            background: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }

        .header {
            text-align: center;
            padding-bottom: 20px;
            border-bottom: 2px solid #eee;
        }

        .content {
            display: none;
            padding: 20px 0;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
            border: 1px solid #ddd;
            border-radius: 8px;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 80%;
        }

        .sidebar {
            position: fixed;
            top: 0;
            left: 0;
            height: 100%;
            width: 250px;
            background-color: #333;
            padding-top: 20px;
        }

        .sidebar a {
            padding: 10px 15px;
            text-decoration: none;
            font-size: 18px;
            color: white;
            display: block;
        }

        .sidebar a:hover {
            background-color: #575757;
        }

        .main-content {
            margin-left: 270px;
            padding: 20px;
        }

        .footer {
            text-align: center;
            margin-top: 20px;
            border-top: 1px solid #eee;
            padding-top: 10px;
        }
    </style>
</head>

<body>

    <div class="sidebar">
        <a href="javascript:void(0)" onclick="showContent('introduction')">Introduction</a>
        <a href="javascript:void(0)" onclick="showContent('installation')">Installation</a>
        <a href="javascript:void(0)" onclick="showContent('usage')">Usage</a>
        <a href="javascript:void(0)" onclick="showContent('background')">Background</a>
        <a href="javascript:void(0)" onclick="showContent('docker')">Docker Containers</a>
        <a href="javascript:void(0)" onclick="showContent('grafana-kafka-prometheus')">Grafana, Kafka, Prometheus</a>
        <a href="javascript:void(0)" onclick="showContent('prototypical-networks')">Prototypical Networks</a>
        <a href="javascript:void(0)" onclick="showContent('license')">License</a>
        <a href="javascript:void(0)" onclick="showContent('citation')">Citation</a>
        <a href="javascript:void(0)" onclick="toggleSubcategories('codeSubcategories')">Code</a>
        <div id="codeSubcategories" style="display:none;">
            <a href="javascript:void(0)" style="padding-left:20px;" onclick="showContent('file1', true)">File 1</a>
            <a href="javascript:void(0)" style="padding-left:20px;" onclick="showContent('file2', true)">File 2</a>                
        </div>
    </div>

    <div class="main-content">
        <div class="header">
            <h1>Smartville</h1>
            <p>This is the official SmartVille repository</p>
            <p>Smartville is an open-source testbed based on GNS3, Pytorch, and Docker for training and testing online
                intrusion detection systems based on machine learning.</p>
            <p>Feel free to contribute!</p>
            <p>The related paper <em>"SmartVille: an open-source SDN online-intrusion detection testbed"</em> is under
                review. Stay tuned!</p>
        </div>

        <div id="introduction" class="content">
            <h2>Introduction</h2>
            <p>Networking environments are composed of a data plane, responsible for forwarding packets, and a
                control plane, responsible for determining how packets are forwarded. To illustrate the difference
                between SDN and traditional networking, consider the following example. If Alice wants to email Bob,
                Alice‚Äôs router will forward the packets (data plane) according to its routing table (control plane)
                to Bob‚Äôs router. In traditional networking, the data and control planes reside on the same device,
                whereas in SDN, a separate "layer" known as the controller is added between the data plane and the
                control plane. The controller acts as centralized intelligence that specifies how the nodes must
                handle the packets.</p>
        </div>

        <div id="installation" class="content">
            <h2>Installation</h2>
            <h3>Setting Up the Program on a Native Linux Distribution or WSL</h3>
            <p>This guide provides step-by-step instructions for setting up the testbed on a native Linux distribution.
                We have not verified if it works on the Windows Subsystem for Linux (WSL).</p>
            <h4>GNS3</h4>
            <p>Install the GNS3 software following the official documentation at: <a
                    href="https://docs.gns3.com/docs/getting-started/installation/linux/">https://docs.gns3.com/docs/getting-started/installation/linux/</a>
            </p>
            <p>Start the server either via GUI or by running:</p>
            <pre>gns3server</pre>
            <p>The server will run on <em>localhost:3080</em> as default.</p>
            <h4>Dependencies</h4>
            <p>All the required dependencies are listed in the <code>requirements.txt</code> file.</p>
            <pre>pip install -r requirements.txt</pre>
            <h4>Docker Images</h4>
            <p>The docker images used to build the nodes can be obtained by running the Makefile:</p>
            <pre>make all</pre>
            <h4>Setup</h4>
            <p>Modify the line 21 in file <code>controller.dockerfile</code> and insert your wandb API key as:</p>
            <pre>your_wandb_api_key.txt</pre>
        </div>

        <div id="usage" class="content">
            <h2>Usage</h2>
            <h3>Build Topology</h3>
            <p>To start the <em>star topology</em> execute <code>star_topology.py</code>:</p>
            <pre>python3 utils/star_topology.py</pre>
            <p>You will get in the GNS3 GUI a new project with this scenario.</p>
            <img src="https://github.com/DISTA-IoT/smartville/blob/release_1.0/readme_imgs/topology.png?raw=true" alt="GNS3 Topology Diagram" class="center">
            <p>Each node can communicate with each other and everyone has Internet connection available.</p>
            <h4>Container Manager </h4>
            <p>Execute the container_manager script and it will guide you through the rest!</p>
            <pre>python3 utils/container_manager.py</pre>
            <p>Stay tuned for a full walk-through tutorial!</p>
            <h4>Modify nodes </h4>
            <p>Each node can be modified or replaced by manipulating the node.dockerfile and star_topology.py to fit the
                desired requirements.</p>
        </div>

        <div id="background" class="content">
            <h2>Background</h2>
            <h3>SDN and OpenFlow characteristics</h3>
            <img src="https://github.com/DISTA-IoT/smartville/blob/release_1.0/readme_imgs/sdn.png?raw=true">
            <h3>Introduction</h3>
            <p>Networking environments are composed of a data plane, responsible for forwarding packets, and a
                control plane, responsible for determining how packets are forwarded. To illustrate the difference
                between SDN and traditional networking, consider the following example. If Alice wants to email Bob,
                Alice‚Äôs router will forward the packets (data plane) according to its routing table (control plane)
                to Bob‚Äôs router. In traditional networking, the data and control planes reside on the same device,
                whereas in SDN, a separate "layer" known as the controller is added between the data plane and the
                control plane. The controller acts as centralized intelligence that specifies how the nodes must
                handle the packets.</p>
            <h3>SDN Interactions</h3>
            <p>There are four types of interactions in SDN. The controller interacts with the application plane
                through the so-called Northbound APIs. Through these APIs, applications communicate network resource
                requisites (data, storage, bandwidth, etc.) so the network can be configured accordingly. Northbound
                APIs should adhere to the REST criteria.</p>
            <p>On the other hand, the controller interacts with the network forwarding elements (data plane) through
                the Southbound APIs, which allow the controller to send commands directly to the nodes. Protocols
                such as OpenFlow and NETCONF were created for this purpose. Third, interactions between SDN and
                traditional networks are made possible via the Westbound APIs that use hybrid solutions such as
                SDN-IP, RouteFlow, and BTSDN. Lastly, interactions from SDN to SDN are made possible via the
                Eastbound APIs that use protocols such as HyperFlow and Onix.</p>
        </div>

        <div id="docker" class="content">
            <h2>Docker Containers</h2>
            <p>A Docker container characterizes each node in the SmartVille testbed. The container images are
                buildable using the corresponding Dockerfiles that handle all the dependencies and the nodes'
                internal file-system structure.</p>
            <p>This example of code represents the Dockerfile of each victim node. It's possible to identify four
                main Docker containers in our testbed:</p>
            <ul>
                <li><strong>Controller:</strong> All the dependencies to run the controller and a suite of
                    networking tools are installed. The POX library and PyTorch are retrieved and set to the
                    gar-experimental branch. Kafka, Grafana, and Prometheus tools are retrieved and ports 9090,
                    9092, and 3000 are exposed to allow the instance of the three servers. Lastly, the script
                    entrypoint.sh, which keeps the application alive, is run.</li>
                <li><strong>Victim:</strong> All the dependencies to run the victim behavior are installed (Python3,
                    TCP Replay, Scapy, Kafka, network tools) and the victim's scripts are imported into the file
                    system.</li>
                <li><strong>Attacker:</strong> All the dependencies to run the attacker behavior are installed
                    (Python3, TCP Replay, Scapy, Kafka, network tools) and the scripts of the different cyberattacks
                    are copied into the file system.</li>
                <li><strong>Switch:</strong> The container of the OpenVSwitch is based on the official GNS3
                    OpenVSwitch appliance. However, the boot kernel has been modified to suit our application
                    scenario via the boot.sh script.</li>
            </ul>
            <p>The Docker containers can be modified to match the configuration of every desired device that needs
                to be emulated in the network topology.</p>

            <h3>OpenFlow Switch</h3>
            <p>The OpenVSwitch is based on the GNS3 appliance, but the boot kernel is modified at launch by the
                boot.sh script:</p>
            <p>The last part of the script sets the switch to listen for a controller on physical port br0 with IP
                address 192.168.1.1 port 6633. This part can be modified to fit the desired IP range case scenario.
            </p>
            <p>OpenFlow switches operate based on a set of high-level commands defined by the OpenFlow protocol,
                providing a flexible and programmable approach to network management. When an OpenFlow controller
                issues OpenFlow commands, such as flow table modifications or routing instructions, the switch
                processes these directives to define its forwarding behavior. The switch maintains a flow table that
                stores rules specifying how to handle incoming packets. Each rule is characterized by a set of
                fields that identify the sender, the receiver, the used communication protocol, and a set of actions
                to perform when the switch receives a packet that matches those fields. Each time a packet is
                received by the switch, it consults its flow table to determine the appropriate action to perform,
                such as forwarding, dropping, or modifying the packet.</p>
            <p>If none of the rules match the packet's parameters, a flow miss occurs. The packet is sent to the
                controller, which has to build and send back an appropriate OpenFlow command to the switch. This
                ensures that the next time a similar packet arrives, it will trigger a flow hit and be handled
                directly by the switch. This paradigm allows for dynamic network behavior adjustments, as the
                controller can remotely instruct switches to adapt to changing network conditions. OpenFlow's
                separation of the control and data planes empowers administrators to centrally manage and
                orchestrate network policies, enabling agility and responsiveness in modern network infrastructures.
            </p>
        </div>

        <div id="grafana-kafka-prometheus" class="content">
            <h2>Grafana, Kafka, Prometheus</h2>
            <p>Apache Kafka is a distributed data streaming platform commonly referred to as a messaging system. It
                is capable of publishing messages, storing, and processing records in real-time. Kafka handles
                immense volumes of data where multiple clients can consume or publish messages on its topics. In our
                testbed, it's used by the nodes that act as producers to send information related to their CPU, RAM,
                and network-related metrics while creating a new topic for each of them. The controller will then
                connect as a consumer to the Kafka server to consume this data.</p>
            <p>Prometheus is a system for monitoring systems and services. It collects metrics from configured
                targets at defined intervals, evaluates rule expressions, displays the results, and can trigger
                alerts when specified conditions are observed. In our work, Prometheus is used to guarantee the
                persistence of data. All the information sent to the Kafka server is saved into a NoSQL database by
                Prometheus.</p>
            <p>Grafana is an interactive open-source data visualization platform developed by Grafana Labs. It
                allows users to view data through unified tables and charts on a single or multiple dashboards,
                making interpretation and understanding easier. In our work, Grafana and Prometheus work in tandem.
                Once the data is saved persistently by Prometheus, it's then retrieved by Grafana to build and show
                a comprehensive dashboard of each topic to the user.</p>
            <img src="https://github.com/DISTA-IoT/smartville/blob/release_1.0/readme_imgs/kafka.png?raw=true">
            <p>As a result of these system interactions, the user can view a dashboard of the devices' status in the
                network. The persistent data is then made available for further usage by the controller.</p>
        </div>

        <div id="prototypical-networks" class="content">
            <h2>Prototypical Networks (PN)</h2>
            <p>Prototypical Networks (PN) offer a neural architectural strategy that decouples the classification
                task from the singular distributions of classes. PN can be considered representational machinery
                that learns a proto-distribution from which every class distribution is generated. Not only do PNs
                achieve learning efficiency, but the inherent geometrical inductive biases produce latent class
                prototypes that can be further used for discriminative tasks.</p>
            <p>PNs are trained through episodic learning: Given in input a set of query and support latent samples,
                PNs make a multiclass classification inference for each one of the former as a function of the
                labels of the latter: Let the input batch be represented by ùìë = {ùìëùì¢ ‚à™ ùìëùì†} where ùìëùì¢ is the set
                of support latent vectors ùê≥ùê¨1, ùê≥ùê¨2, ‚Ä¶, ùê≥ùê¨|ùìëùì¢| and ùìëùì† is the set of query latent vectors
                ùê≥ùê™1, ùê≥ùê™2, ‚Ä¶, ùê≥ùê™|ùìëùì†|. The class-wise centroids or prototypes are computed using the support
                latents:</p>
            <img src="https://github.com/DISTA-IoT/smartville/blob/release_1.0/readme_imgs/formula.png?raw=true" class="center">
            <p>Where ùëÅ<sub>ùëñ</sub> is the number of support latents in class ùëñ and ùìí is the set of classes
                included in ùìë.</p>
            <p>Successively, PNs build a classification logits vector for each query sample where the vector
                components are the association scores to each class. These scores are inversely proportional to the
                Euclidean distances between the latent representation of the query sample and the correspondent
                class prototype. The neural modules of the SmartController implemented in SmartVille are those of
                ASAP that use PNs to perform multi-class classification of attacks. By doing so, the class
                prototypes learnt in the PN framework are mapped to latent attack signatures. For more information
                on the prototypical classification mechanism in our neural modules, the reader is referred to ASAP.
            </p>
        </div>

        <div id="license" class="content">
            <h1>License</h1>
            <p><strong>Apache License Version 2.0, January 2004</strong> <a href="http://www.apache.org/licenses/">http://www.apache.org/licenses/</a></p>
            <h2>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</h2>
            <h3>Definitions.</h3>
            <p><strong>"License"</strong> shall mean the terms and conditions for use, reproduction, and
                distribution as defined by Sections 1 through 9 of this document.</p>
            <p><strong>"Licensor"</strong> shall mean the copyright owner or entity authorized by the copyright
                owner that is granting the License.</p>
            <p><strong>"Legal Entity"</strong> shall mean the union of the acting entity and all other entities that
                control, are controlled by, or are under common control with that entity. For the purposes of this
                definition, "control" means (i) the power, direct or indirect, to cause the direction or management
                of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more
                of the outstanding shares, or (iii) beneficial ownership of such entity.</p>
            <p><strong>"You" (or "Your")</strong> shall mean an individual or Legal Entity exercising permissions
                granted by this License.</p>
            <p><strong>"Source" form</strong> shall mean the preferred form for making modifications, including but
                not limited to software source code, documentation source, and configuration files.</p>
            <p><strong>"Object" form</strong> shall mean any form resulting from mechanical transformation or
                translation of a Source form, including but not limited to compiled object code, generated
                documentation, and conversions to other media types.</p>
            <p><strong>"Work"</strong> shall mean the work of authorship, whether in Source or Object form, made
                available under the License, as indicated by a copyright notice that is included in or attached to
                the work (an example is provided in the Appendix below).</p>
            <p><strong>"Derivative Works"</strong> shall mean any work, whether in Source or Object form, that is
                based on (or derived from) the Work and for which the editorial revisions, annotations,
                elaborations, or other modifications represent, as a whole, an original work of authorship. For the
                purposes of this License, Derivative Works shall not include works that remain separable from, or
                merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p>
            <p><strong>"Contribution"</strong> shall mean any work of authorship, including the original version of
                the Work and any modifications or additions to that Work or Derivative Works thereof, that is
                intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an
                individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes
                of this definition, "submitted" means any form of electronic, verbal, or written communication sent
                to the Licensor or its representatives, including but not limited to communication on electronic
                mailing lists, source code control systems, and issue tracking systems that are managed by, or on
                behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding
                communication that is conspicuously marked or otherwise designated in writing by the copyright owner
                as "Not a Contribution."</p>
            <p><strong>"Contributor"</strong> shall mean Licensor and any individual or Legal Entity on behalf of
                whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>
            <h3>Grant of Copyright License.</h3>
            <p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a
                perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to
                reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and
                distribute the Work and such Derivative Works in Source or Object form.</p>
            <h3>Grant of Patent License.</h3>
            <p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a
                perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this
                section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer
                the Work, where such license applies only to those patent claims licensable by such Contributor that
                are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s)
                with the Work to which such Contribution(s) was submitted. If You institute patent litigation
                against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or
                a Contribution incorporated within the Work constitutes direct or contributory patent infringement,
                then any patent licenses granted to You under this License for that Work shall terminate as of the
                date such litigation is filed.</p>
            <h3>Redistribution.</h3>
            <p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with
                or without modifications, and in Source or Object form, provided that You meet the following
                conditions:</p>
            <ul>
                <li>(a) You must give any other recipients of the Work or Derivative Works a copy of this License;
                    and</li>
                <li>(b) You must cause any modified files to carry prominent notices stating that You changed the
                    files; and</li>
                <li>(c) You must retain, in the Source form of any Derivative Works that You distribute, all
                    copyright, patent, trademark, and attribution notices from the Source form of the Work,
                    excluding those notices that do not pertain to any part of the Derivative Works; and</li>
                <li>(d) If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative
                    Works that You distribute must include a readable copy of the attribution notices contained
                    within such NOTICE file, excluding those notices that do not pertain to any part of the
                    Derivative Works, in at least one of the following places: within a NOTICE text file distributed
                    as part of the Derivative Works; within the Source form or documentation, if provided along with
                    the Derivative Works; or, within a display generated by the Derivative Works, if and wherever
                    such third-party notices normally appear. The contents of the NOTICE file are for informational
                    purposes only and do not modify the License. You may add Your own attribution notices within
                    Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the
                    Work, provided that such additional attribution notices cannot be construed as modifying the
                    License.</li>
            </ul>
            <p>You may add Your own copyright statement to Your modifications and may provide additional or
                different license terms and conditions for use, reproduction, or distribution of Your modifications,
                or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of
                the Work otherwise complies with the conditions stated in this License.</p>
            <h3>Submission of Contributions.</h3>
            <p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the
                Work by You to the Licensor shall be under the terms and conditions of this License, without any
                additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify
                the terms of any separate license agreement you may have executed with Licensor regarding such
                Contributions.</p>
            <h3>Trademarks.</h3>
            <p>This License does not grant permission to use the trade names, trademarks, service marks, or product
                names of the Licensor, except as required for reasonable and customary use in describing the origin
                of the Work and reproducing the content of the NOTICE file.</p>
            <h3>Disclaimer of Warranty.</h3>
            <p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each
                Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
                KIND, either express or implied, including, without limitation, any warranties or conditions of
                TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely
                responsible for determining the appropriateness of using or redistributing the Work and assume any
                risks associated.</p>
        </div>

        <div id="citation" class="content">
            <h2>Citation</h2>
            <pre>
                    @misc{cevallos2024smartville,
                        title = {Smartville: an open-source SDN online-intrusion detection testbed},
                        author = {Cevallos, Jesus and Iannello, Stefano and Grattacaso, Giuseppe and Rizzardi, Alessandra and Sicari, Sabrina and Coen-Porisini, Alberto},
                        year = {2024}, 
                        url = {https://github.com/DISTA-IoT/smartville},
                        note = {Accessed: YYYY-MM-DD} 
                        }
                </pre>
        </div>
        <div id="code" class="content">
            <h2>Code</h2>
            <p>Select a file from the list:</p>
            <!-- Subcategories will be shown here -->
        </div>
        
        <div id="file1" class="content">
            <h2>File 1</h2>
            <h3>Class: RecurrentModel</h3>
            <p>The <code>RecurrentModel</code> class is a recurrent model that uses a GRU (Gated Recurrent Unit) to process data sequences.</p>
            <pre><code>
        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        
        class RecurrentModel(nn.Module):
            def __init__(self, input_size, hidden_size, device='cpu'):
                super(RecurrentModel, self).__init__()
                self.device = device
                self.hidden_size = hidden_size
                self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        
            def forward(self, x):
                h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
                out, _ = self.gru(x, h0)
                return F.relu(out[:, -1, :])
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, input_size, hidden_size, device='cpu')</code>: Initializes the model with input size, hidden state size, and device.</li>
                <li><code>forward(self, x)</code>: Defines the forward pass of the model, which computes the GRU output and applies a ReLU activation function.</li>
            </ul>
        
            <h3>Class: MulticlassPrototypicalClassifier</h3>
            <p>The <code>MulticlassPrototypicalClassifier</code> class is a classifier that uses prototypes for multiclass classification.</p>
            <pre><code>
        class MulticlassPrototypicalClassifier(nn.Module):
            def __init__(self, device='cpu'):
                super(MulticlassPrototypicalClassifier, self).__init__()
                self.device = device
        
            def get_oh_labels(self, decimal_labels, n_way):
                labels_onehot = torch.zeros([decimal_labels.size()[0], n_way], device=self.device)
                labels_onehot = labels_onehot.scatter(1, decimal_labels, 1)
                return labels_onehot
        
            def get_centroids(self, hidden_vectors, onehot_labels):
                cluster_agg = onehot_labels.T @ hidden_vectors
                samples_per_cluster = onehot_labels.sum(0)
                centroids = torch.zeros_like(cluster_agg, device=self.device)
                missing_clusters = samples_per_cluster == 0
                existent_centroids = cluster_agg[~missing_clusters] / samples_per_cluster[~missing_clusters].unsqueeze(-1)
                centroids[~missing_clusters] = existent_centroids
                return centroids, missing_clusters
        
            def forward(self, hidden_vectors, labels, known_attacks_count, query_mask):
                oh_labels = self.get_oh_labels(decimal_labels=labels.long(), n_way=known_attacks_count)
                centroids, _ = self.get_centroids(hidden_vectors[~query_mask], oh_labels[~query_mask])
                scores = 1 / (torch.cdist(hidden_vectors[query_mask], centroids) + 1e-10)
                return scores
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, device='cpu')</code>: Initializes the classifier with the specified device.</li>
                <li><code>get_oh_labels(self, decimal_labels, n_way)</code>: Generates one-hot encoded labels from decimal labels.</li>
                <li><code>get_centroids(self, hidden_vectors, onehot_labels)</code>: Computes centroids of clusters.</li>
                <li><code>forward(self, hidden_vectors, labels, known_attacks_count, query_mask)</code>: Defines the forward pass of the model, calculating scores based on centroids and distances.</li>
            </ul>
        
            <h3>Class: MultiClassFlowClassifier</h3>
            <p>The <code>MultiClassFlowClassifier</code> class is a classifier that uses multiclass data flows.</p>
            <pre><code>
        class MultiClassFlowClassifier(nn.Module):
            def __init__(self, input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu'):
                super(MultiClassFlowClassifier, self).__init__()
                self.device = device
                self.normalizer = nn.BatchNorm1d(input_size)
                self.rnn = RecurrentModel(input_size, hidden_size, device=self.device)
                self.kernel_regressor = HighDimKernelRegressor(in_features=hidden_size, out_features=hidden_size, n_heads=kr_heads, dropout=dropout_prob, device=self.device)
                self.classifier = MulticlassPrototypicalClassifier(device=self.device)
        
            def forward(self, x, labels, curr_known_attack_count, query_mask):
                x = self.normalizer(x.permute((0,2,1))).permute((0,2,1))
                hiddens = self.rnn(x)
                hiddens, predicted_kernel = self.kernel_regressor(hiddens)
                logits = self.classifier(hiddens, labels, curr_known_attack_count, query_mask)
                return logits, hiddens, predicted_kernel
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu')</code>: Initializes the classifier with input size, hidden state size, dropout probability, and number of heads for the kernel regressor.</li>
                <li><code>forward(self, x, labels, curr_known_attack_count, query_mask)</code>: Defines the forward pass of the model, normalizing the input, passing through the RNN model and kernel regressor, and finally classifying using the prototypical classifier.</li>
            </ul>
        
            <h3>Class: TwoStreamMulticlassFlowClassifier</h3>
            <p>The <code>TwoStreamMulticlassFlowClassifier</code> class is a classifier that uses two streams of multiclass data.</p>
            <pre><code>
        class TwoStreamMulticlassFlowClassifier(nn.Module):
            def __init__(self, flow_input_size, second_stream_input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu'):
                super(TwoStreamMulticlassFlowClassifier, self).__init__()
                self.device = device
                self.flow_normalizer = nn.BatchNorm1d(flow_input_size)
                self.flow_rnn = RecurrentModel(flow_input_size, hidden_size, device=self.device)
                self.second_stream_normalizer = nn.BatchNorm1d(second_stream_input_size)
                self.second_stream_rnn = RecurrentModel(second_stream_input_size, hidden_size, device=self.device)
                self.kernel_regressor = HighDimKernelRegressor(in_features=hidden_size*2, out_features=hidden_size, n_heads=kr_heads, dropout=dropout_prob, device=self.device)
                self.classifier = MulticlassPrototypicalClassifier(device=self.device)
        
            def forward(self, flows, second_domain_feats, labels, curr_known_attack_count, query_mask):
                flows = self.flow_normalizer(flows.permute((0,2,1))).permute((0,2,1))
                second_domain_feats = self.second_stream_normalizer(second_domain_feats.permute((0,2,1))).permute((0,2,1))
        
                flows = self.flow_rnn(flows)
                second_domain_feats = self.second_stream_rnn(second_domain_feats)
        
                hiddens = torch.cat([flows, second_domain_feats], dim=1)
        
                hiddens, predicted_kernel = self.kernel_regressor(hiddens)
                logits = self.classifier(hiddens, labels, curr_known_attack_count, query_mask)
        
                return logits, hiddens, predicted_kernel
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, flow_input_size, second_stream_input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu')</code>: Initializes the classifier with two input streams, hidden state size, dropout probability, and number of heads for the kernel regressor.</li>
                <li><code>forward(self, flows, second_domain_feats, labels, curr_known_attack_count, query_mask)</code>: Defines the forward pass of the model, normalizing the input streams, passing through the RNN models and kernel regressor, and finally classifying using the prototypical classifier.</li>
            </ul>
        
            <h3>Class: ThreeStreamMulticlassFlowClassifier</h3>
            <p>The <code>ThreeStreamMulticlassFlowClassifier</code> class is a classifier that uses three streams of multiclass data.</p>
            <pre><code>
        class ThreeStreamMulticlassFlowClassifier(nn.Module):
            def __init__(self, flow_input_size, second_stream_input_size, third_stream_input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu'):
                super(ThreeStreamMulticlassFlowClassifier, self).__init__()
                self.device = device
                self.flow_normalizer = nn.BatchNorm1d(flow_input_size)
                self.flow_rnn = RecurrentModel(flow_input_size, hidden_size, device=self.device)
                self.second_stream_normalizer = nn.BatchNorm1d(second_stream_input_size)
                self.second_stream_rnn = RecurrentModel(second_stream_input_size, hidden_size, device=self.device)
                self.third_stream_normalizer = nn.BatchNorm1d(third_stream_input_size)
                self.third_stream_rnn = RecurrentModel(third_stream_input_size, hidden_size, device=self.device)
                self.kernel_regressor = HighDimKernelRegressor(in_features=hidden_size*3, out_features=hidden_size, n_heads=kr_heads, dropout=dropout_prob, device=self.device)
                self.classifier = MulticlassPrototypicalClassifier(device=self.device)
        
            def forward(self, flows, second_domain_feats, third_domain_feats, labels, curr_known_attack_count, query_mask):
                flows = self.flow_normalizer(flows.permute((0,2,1))).permute((0,2,1))
                second_domain_feats = self.second_stream_normalizer(second_domain_feats.permute((0,2,1))).permute((0,2,1))
                if torch.any(third_domain_feats.isnan()):
                    print('hello')
                third_domain_feats = self.third_stream_normalizer(third_domain_feats.permute((0,2,1))).permute((0,2,1))
        
                flows = self.flow_rnn(flows)
                second_domain_feats = self.second_stream_rnn(second_domain_feats)
                third_domain_feats = self.third_stream_rnn(third_domain_feats)
        
                hiddens = torch.cat([flows, second_domain_feats, third_domain_feats], dim=1)
        
                hiddens, predicted_kernel = self.kernel_regressor(hiddens)
                logits = self.classifier(hiddens, labels, curr_known_attack_count, query_mask)
        
                return logits, hiddens, predicted_kernel
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, flow_input_size, second_stream_input_size, third_stream_input_size, hidden_size, dropout_prob=0.2, kr_heads=8, device='cpu')</code>: Initializes the classifier with three input streams, hidden state size, dropout probability, and number of heads for the kernel regressor.</li>
                <li><code>forward(self, flows, second_domain_feats, third_domain_feats, labels, curr_known_attack_count, query_mask)</code>: Defines the forward pass of the model, normalizing the input streams, passing through the RNN models and kernel regressor, and finally classifying using the prototypical classifier.</li>
            </ul>
        
            <h3>Class: ConfidenceDecoder</h3>
            <p>The <code>ConfidenceDecoder</code> class is a confidence decoder that transforms confidence scores into unknown indicators.</p>
            <pre><code>
        class ConfidenceDecoder(nn.Module):
            def __init__(self, device):
                super(ConfidenceDecoder, self).__init__()
                self.device = device
        
            def forward(self, scores):
                scores = (1 - scores.unsqueeze(-1)).min(1)[0]
                unknown_indicators = torch.sigmoid(scores)
                return unknown_indicators
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, device)</code>: Initializes the decoder with the specified device.</li>
                <li><code>forward(self, scores)</code>: Transforms confidence scores into unknown indicators using a sigmoid function.</li>
            </ul>
        
            <h3>Class: KernelRegressionLoss</h3>
            <p>The <code>KernelRegressionLoss</code> class computes the loss for kernel regression.</p>
            <pre><code>
        class KernelRegressionLoss(nn.Module):
            def __init__(self, repulsive_weigth=1, attractive_weigth=1, device='cpu'):
                super(KernelRegressionLoss, self).__init__()
                self.r_w = repulsive_weigth
                self.a_w = attractive_weigth
                self.device = device
        
            def forward(self, baseline_kernel, predicted_kernel):
                repulsive_CE_term = -(1 - baseline_kernel) * torch.log(1-predicted_kernel + 1e-10)
                repulsive_CE_term = repulsive_CE_term.sum(dim=1)
                repulsive_CE_term = repulsive_CE_term.mean()
        
                attractive_CE_term = -(baseline_kernel * torch.log(predicted_kernel + 1e-10))
                attractive_CE_term = attractive_CE_term.sum(dim=1)
                attractive_CE_term = attractive_CE_term.mean()
        
                return (self.r_w * repulsive_CE_term) + (self.a_w * attractive_CE_term)
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, repulsive_weigth=1, attractive_weigth=1, device='cpu')</code>: Initializes the loss calculator with specified repulsive and attractive weights and device.</li>
                <li><code>forward(self, baseline_kernel, predicted_kernel)</code>: Computes the loss for kernel regression by combining repulsive and attractive forces.</li>
            </ul>
        
            <h3>Class: SimmilarityNet</h3>
            <p>The <code>SimmilarityNet</code> class computes the similarity between two vectors.</p>
            <pre><code>
        class SimmilarityNet(nn.Module):
            def __init__(self, h_dim):
                super(SimmilarityNet, self).__init__()
                self.act = nn.LeakyReLU(0.2)
                self.fc1 = nn.Linear(h_dim, h_dim // 2)
                self.fc2 = nn.Linear(h_dim // 2, 1)
        
            def forward(self, x1, x2):
                input_to_symm = torch.abs(x1 - x2)
                symm = self.fc1(input_to_symm)
                symm = self.act(symm)
                symm = self.fc2(symm)
                return symm
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, h_dim)</code>: Initializes the similarity network with the dimension of the input vectors.</li>
                <li><code>forward(self, x1, x2)</code>: Computes the similarity between two vectors using a two-layer neural network with LeakyReLU activation.</li>
            </ul>
        
            <h3>Class: HighDimKernelRegressor</h3>
            <p>The <code>HighDimKernelRegressor</code> class is a high-dimensional kernel regressor that uses a similarity network.</p>
            <pre><code>
        class HighDimKernelRegressor(nn.Module):
            def __init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu'):
                super(HighDimKernelRegressor, self).__init__()
                self.device = device
                self.w = nn.Parameter(torch.tensor(1.0))
                self.b = nn.Parameter(torch.tensor(-0.5))
                self.similarity_network = SimmilarityNet(h_dim=in_features)
        
            def forward(self, hiddens):
                n_nodes = hiddens.shape[0]
                h_pivot = hiddens.repeat(n_nodes, 1)
                h_interleave = hiddens.repeat_interleave(n_nodes, dim=0)
                energies = self.similarity_network(h_pivot, h_interleave)
                kernel = torch.sigmoid(energies)
                kernel = kernel.reshape(n_nodes, n_nodes)
                return hiddens, kernel
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu')</code>: Initializes the regressor with the specified parameters and a similarity network.</li>
                <li><code>forward(self, hiddens)</code>: Defines the forward pass of the kernel regressor, computing kernels and similarity energies between hidden vectors.</li>
            </ul>
        
            <h3>Class: SimpleKernelRegressor</h3>
            <p>The <code>SimpleKernelRegressor</code> class is a simple kernel regressor that computes distance-based kernels.</p>
            <pre><code>
        class SimpleKernelRegressor(nn.Module):
            def __init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu'):
                super(SimpleKernelRegressor, self).__init__()
                self.device = device
                self.w = nn.Parameter(torch.tensor(1.0))
                self.b = nn.Parameter(torch.tensor(-0.5))
        
            def forward(self, hiddens):
                n_nodes = hiddens.shape[0]
                energies = 1 / (torch.cdist(hiddens, hiddens) + 1e-10)
                kernel = torch.sigmoid(energies)
                kernel = kernel.reshape(n_nodes, n_nodes)
                return hiddens, kernel
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu')</code>: Initializes the simple regressor with the specified parameters.</li>
                <li><code>forward(self, hiddens)</code>: Defines the forward pass of the kernel regressor, computing distance-based kernels between hidden vectors.</li>
            </ul>
        
            <h3>Class: KernelRegressor</h3>
            <p>The <code>KernelRegressor</code> class is a kernel regressor that uses a graph attention layer (Graph Attention Layer).</p>
            <pre><code>
        class KernelRegressor(nn.Module):
            def __init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu'):
                super(KernelRegressor, self).__init__()
                self.regressor = GraphAttentionV2Layer(in_features=in_features, out_features=out_features, n_heads=n_heads, is_concat=is_concat, dropout=dropout, leaky_relu_negative_slope=leaky_relu_negative_slope, share_weights=share_weights)
                self.device = device
        
            def forward(self, hiddens):
                return self.regressor(hiddens)
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.0, leaky_relu_negative_slope=0.2, share_weights=True, device='cpu')</code>: Initializes the kernel regressor with a graph attention layer.</li>
                <li><code>forward(self, hiddens)</code>: Defines the forward pass of the kernel regressor, using a graph attention layer to compute kernels.</li>
            </ul>
        
            <h3>Class: GraphAttentionV2Layer</h3>
            <p>The <code>GraphAttentionV2Layer</code> class is a graph attention layer that computes hidden vectors and kernels for graphs.</p>
            <pre><code>
        class GraphAttentionV2Layer(nn.Module):
            def __init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.1, leaky_relu_negative_slope=0.2, share_weights=True):
                super().__init__()
                self.is_concat = is_concat
                self.n_heads = n_heads
                self.share_weights = share_weights
        
                if is_concat:
                    assert out_features % n_heads == 0
                    self.n_hidden = out_features // n_heads
                else:
                    self.n_hidden = out_features
        
                self.linear_l = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)
                if share_weights:
                    self.linear_r = self.linear_l
                else:
                    self.linear_r = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)
        
                self.attn = nn.Linear(self.n_hidden, 1, bias=False)
                self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)
                self.softmax = nn.Softmax(dim=1)
                self.dropout = nn.Dropout(dropout)
        
            def forward(self, h):
                n_nodes = h.shape[0]
                g_l = self.linear_l(h).view(n_nodes, self.n_heads, self.n_hidden)
                g_r = self.linear_r(h).view(n_nodes, self.n_heads, self.n_hidden)
        
                g_l_repeat = g_l.repeat(n_nodes, 1, 1)
                g_r_repeat_interleave = g_r.repeat_interleave(n_nodes, dim=0)
                g_sum = g_l_repeat + g_r_repeat_interleave
                g_sum = g_sum.view(n_nodes, n_nodes, self.n_heads, self.n_hidden)
        
                e = self.attn(self.activation(g_sum))
                e = e.squeeze(-1)
                a = self.softmax(e)
                a = self.dropout(a)
        
                hiddens = torch.einsum('ijh,jhf->ihf', a, g_r)
        
                if self.is_concat:
                    hiddens = hiddens.reshape(n_nodes, self.n_heads * self.n_hidden)
                else:
                    hiddens = hiddens.mean(dim=1)
        
                a = a.mean(dim=2)
                a = a / (a.max(dim=1)[0] + 1e-10)
                a = a.clamp(min=0, max=1)
        
                return hiddens, a
            </code></pre>
            <p><strong>Methods:</strong></p>
            <ul>
                <li><code>__init__(self, in_features, out_features, n_heads, is_concat=False, dropout=0.1, leaky_relu_negative_slope=0.2, share_weights=True)</code>: Initializes the graph attention layer with the specified parameters.</li>
                <li><code>forward(self, h)</code>: Defines the forward pass of the graph attention layer, computing hidden vectors and kernels for graphs.</li>
            </ul>
        </div>
        
        <div id="file2" class="content">
            <h2>File 2</h2>
            <p>Content for File 2...</p>
        </div>
        

    <script>
        function showContent(sectionId) {
            // Hide all content sections
            var contents = document.getElementsByClassName('content');
            for (var i = 0; i < contents.length; i++) {
                contents[i].style.display = 'none';
            }

            // Show the selected content section
            var selectedContent = document.getElementById(sectionId);
            if (selectedContent) {
                selectedContent.style.display = 'block';
            }
        }

        function showContent(sectionId, keepSubcategories) {
            // Hide all content sections
            var contents = document.getElementsByClassName('content');
            for (var i = 0; i < contents.length; i++) {
                contents[i].style.display = 'none';
            }

            // Show the selected content section
            var selectedContent = document.getElementById(sectionId);
            if (selectedContent) {
                selectedContent.style.display = 'block';
            }

            // Show or hide the code subcategories
            if (sectionId === 'code' || keepSubcategories) {
                document.getElementById('codeSubcategories').style.display = 'block';
            } else {
                document.getElementById('codeSubcategories').style.display = 'none';
            }
        }

        function toggleSubcategories(subcategoryId) {
            var subcategories = document.getElementById(subcategoryId);
            if (subcategories.style.display === 'block') {
                subcategories.style.display = 'none';
            } else {
                subcategories.style.display = 'block';
            }
        }
    </script>

</body>

</html>
